# Project overview

The goal of this project is to build an active learning pipeline for training a YOLOv11 model using the Ultralytics API. The workflow begins with a collection of unlabeled images. A small subset of images is manually annotated, a model is trained, and the trained model then suggests the next set of images to annotate based on uncertainty or entropy.

The project should provide:

- An annotation workflow integrated directly into the repository.
- Utilities to store labels and convert them into a YOLO-format dataset.
- Dataset generation logic that maintains balanced class distributions across training and validation splits.
- Active learning support where the model identifies low-confidence or high-uncertainty images for further labeling.

In addition, the project should explore using Lightly Train for self-supervised pretraining of the YOLO backbone using unlabeled images. The distilled backbone can then be used for downstream supervised training, with experiments to evaluate whether this improves model accuracy.

# Repository overview

Raw footage/

- Directory containing all raw, unlabeled images.

Labels/ 

- Contains YOLO-format label files.
- One `.txt` file per image, storing bounding boxes and class IDs.

dataset/

- Generated YOLO dataset.
- Contains `train/` and `val/` splits with corresponding images and labels.

Data config:

- YAML file mapping class IDs to human-readable class names.

Train config: 

- YAML file defining YOLO training parameters and configuration for self-supervised training

App config:

- YAML file containing application-level settings, including:
    - Paths to raw images and labels
    - Output dataset directory
    - Train/validation split ratio
    - Number of images to suggest for annotation per iteration

low_confidence_images.txt

- Generated file listing images sorted by uncertainty (e.g., entropy).
- Used to drive the next annotation iteration.

## Scripts

## Generate Dataset

This script is responsible for converting labeled images into a YOLO-formatted dataset.

Functionality:

- Parse all available labels and compute the overall class distribution.
- Use the train/validation split ratio defined in the app config.
- Allocate images to the validation set in a way that preserves class balance as much as possible (initially using a greedy heuristic; improved strategies can be explored later).
- Assign remaining images to the training set.
- Copy images and labels into the output dataset directory specified in the app config.

## Train Model

Handles model training using the Ultralytics YOLO API.

Functionality:

- Train a YOLO model using parameters defined in the train config.
- Use the dataset path generated by the dataset script.
- Optionally run self-supervised training using the Lightly API to generate a distilled backbone.
- Support switching between standard training and distilled-backbone training via an argparse flag (e.g. `--backbone`).
- Initially assume backbone training occurs once per experiment, but leave room for iterative experimentation.

## Analyze Images

Responsible for identifying the most informative images to label next.

Functionality:

- Load the evaluation model specified in the app config.
- Convert the model to a TensorRT (TRT) engine for faster inference.
- Run inference on all (or selected) images.
- Compute uncertainty metrics such as confidence, entropy, or similar.
- Extract feature embeddings and perform K-means clustering.
- From each cluster, select the image with the highest uncertainty to avoid redundancy (e.g., multiple near-identical video frames).
- Output the selected image paths to `low_confidence_images.txt`.
- The number of selected images is controlled via the app config.

## Label Images

This is the primary user-facing component and will be implemented as a Tkinter-based GUI application.

### Core Annotation Features

- Load images from the raw footage directory.
- If a corresponding label file exists, load and display bounding boxes.
- Allow users to:
    - Adjust bounding box positions and sizes
    - Change class IDs for boxes
    - Delete boxes (e.g., via backspace)
    - Create new boxes via click-and-drag
- Class IDs should be loaded from the data config and selectable via keyboard shortcuts or a dropdown menu.
- Provide a “background image” option that marks an image as having no objects.
- Automatically save or update the label file whenever the user clicks “Save.”

### Active Learning Mode

- Provide a toggle that switches the image source from raw images to those listed in `low_confidence_images.txt`.
- In this mode, the user is prompted to annotate model-suggested images first.

### Model-Suggested Labels

- Include a button to load model-predicted bounding boxes.
- Display suggested boxes in a distinct color.
- Allow users to modify, accept, or delete suggested boxes.
- Toggle to have this load in on images that don't have any existing labels.

### Additional UI Features

- Menu options for:
    - Generating the dataset
    - Running the uncertainty analysis script
    - Triggering model training
- Keyboard and mouse shortcuts to streamline annotation workflow.
- Make it so the user can zoom in and out of images, automatically adjusting the size based on the window size.
- Ability to jump to the next unlabeled image.
