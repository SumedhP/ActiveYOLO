# Project overview

The goal of this project is to build an active learning pipeline for training a YOLOv11 model using the Ultralytics API. The workflow begins with a collection of unlabeled images. On these images, a self-supervised backbone is trained to effectively cluster these images in an embedded space. From this, we select a subset of images to manually annotate, a model is trained, and then we select another set of images to annotate prioritizing clusters with the least coverage or diversity.

The project should provide:

- An annotation workflow integrated directly into the repository.
- Utilities to store labels and convert them into a YOLO-format dataset.
- Dataset generation logic
- Active learning support where new images are added suggested based on embeddings

This project explores using Lightly Train for self-supervised pretraining of the YOLO backbone using unlabeled images. The distilled backbone can then be used for embedding images as well as fine-tuned supervised training, with experiments to evaluate whether this improves framework improves model accuracy compared to the pretrained COCO models provided by ultralytics.

This project works off a user provided validation set to ensure consistent benchmarking of model performance as more images are annotated.

# Repository overview

images/

imageset/

validation_set/

embeddings.csv - A CSV file representing a image file path, cluster id, and embedding

Labels/

imageset/

validation_set/

- Contains YOLO-format label files
- One `.txt` file per image, storing bounding boxes and class IDs.

dataset/

- Generated YOLO dataset.
- Contains `train/` and `val/` splits with corresponding images and labels.

Configs/

Data config:

- YAML file mapping class IDs to human-readable class names.

Train config:

- YAML file defining YOLO training parameters and configuration for self-supervised training

App config:

- YAML file containing application-level settings, including:
    - Paths to images and labels
    - Output dataset directory
    - Number of clusters for embedding space
    - Number of images to suggest for annotation per iteration

active_learning_images.txt

- Generated file listing images used to drive the next annotation iteration.

## Scripts

## Generate Dataset

This script is responsible for converting labeled images into a YOLO-formatted dataset.

Functionality:

- Parse all available labels
- Allocates images into training and validation splits based on manually annotated data and user provided validation split
- Copy images and labels into the output dataset directory specified in the app config.

## Train Model

Handles model training using the Ultralytics YOLO API.

Functionality:

- Train a YOLO model using parameters defined in the train config.
- Use the dataset path generated by the dataset script.
- Optionally run self-supervised training using the Lightly API to generate a distilled backbone.
- Support switching between standard training and distilled-backbone training via an argparse flag (e.g. `--ssl`).
- Initially assume backbone training occurs once per experiment, but leave room for iterative experimentation.

## Embed

Responsible for generating embeddings

Functionality:

- Loads in model
- Run prediction on all images in imageset
- Store the embedding value for this image in `embeddings.csv`

## Active Learning Update

Responsible for identifying the most informative images to label next.

Functionality:

- Loads in image embeddings
- Computes % of cluster coverage per cluster, prioritizing clusters with lower coverage
- Extracts random unlabeled image from cluster to suggest for next iteration using round robin approach from all clusters
- Output the selected image paths to `active_learning_images.txt`.
- The number of selected images is controlled via the app config.

## Label Images

This is the primary user-facing component and will be implemented as a Tkinter-based GUI application.

### Core Annotation Features

- Load images from the raw footage directory.
- If a corresponding label file exists, load and display bounding boxes.
- Allow users to:
    - Adjust bounding box positions and sizes
    - Change class IDs for boxes
    - Delete boxes (e.g., via backspace)
    - Create new boxes via click-and-drag
- Class IDs should be loaded from the data config and selectable via keyboard shortcuts or a dropdown menu.
- Provide a “background image” option that marks an image as having no objects.
- Automatically save or update the label file whenever the user clicks “Save.”

### Active Learning Mode

- Provide a toggle that switches the image source from raw images to those listed in `active_learning_images.txt`.
- In this mode, the user is prompted to annotate model-suggested images first.

### Model-Suggested Labels

- Include a button to load model-predicted bounding boxes.
- Display suggested boxes in a distinct color.
- Allow users to modify, accept, or delete suggested boxes.
- Toggle to have this load in on images that don't have any existing labels.
- When run a model on the image, it should run at the full image resolution to maximize recall (imgsz parameter in Ultralytics API).

### Additional UI Features

- Keyboard and mouse shortcuts to streamline annotation workflow.
- Make it so the user can zoom in and out of images, automatically adjusting the size based on the window size.
- Ability to jump to the next unlabeled image.
